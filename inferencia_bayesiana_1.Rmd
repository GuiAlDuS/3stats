---
title: "Introducción a la inferencia bayesiana"
author: "Guillermo A. Durán"
date: "28 de setiembre de 2021"
output:
  html_document:
    df_print: paged
---

```{r message=FALSE, warning=FALSE}
library(rethinking)
set.seed(1234)
```

Este notebook es una introducción práctica muy general sobre la inferencia bayesiana. Presenta la forma en que me habría gustado recibir una introducción a la inferencia bayesiana y hace eco al enfoque pedagógico del libro Statistical Rethinking.

## Interpretaciones (muy generales) de probabilidad:

-   Según el enfoque frecuentista: probabilidad es la proporción de un evento específico en una serie larga de observaciones/experimentos.

-   Según el enfoque bayesiano: probabilidad es un valor entre 0 y 1 que cuantifica nuestra incertidumbre sobre un fenómeno.

## El modelo generador de datos:

El modelo generador de datos es una función capaz de producir valores.

Usamos la distribución normal como ejemplo de cómo generar un valor desde un modelo generador de datos. En este caso la distribución normal es nuestro modelo generador de datos.

```{r}
t <- rnorm(1, mean = 7, sd = 1.5)
t
```

Nuestro valor *t* es generado por una función con parámetros conocidos (los parámetros en el caso de una distribución normal son el promedio y desviación estándard). Cada vez que creamos un valor desde un modelo generador de datos, este valor fluctuará según las características propias del modelo.

El modelo generador de datos también nos permite tomar el camino opuesto: Dado un modelo generador de datos con parámetros establecidos, ¿qué tan probable es que un valor específico sea generado por él?. A esta estimación se le llama *verosimilitud* y se hablará de ella al final del notebook.

```{r}
dnorm(t, mean = 7, sd = 1.5)
```

### Generando nuestros datos: siendo los dueños de nuestro universo

Ahora en vez de crear un único valor, vamos a crear un vector con 400 valores desde nuestro modelo generador de datos.

Si exploramos ese vector vemos cómo se reflejan las características (parámetros: promedio y desviación estándard) de nuestro modelo generador de datos.

```{r}
n = 400

w <- rnorm(n, mean = 7, sd = 1.5)

hist(w, breaks = 20)
cat(
  paste0("Promedio: ", round(mean(w), 2), 
         "\nDesviación estándard: ", round(sd(w), 2)))

```

#### Un ejemplo usando nuestro modelo generador de datos

Ahora pensemos que en nuestro universo la altura de una planta está dada por dos factores (variables) en una relación según la siguiente ecuación:

$$
y = b_1w + b_2z
$$

donde:

$y$ = altura de la planta

$w$ = cantidad de agua

$b_1$ = coeficiente que nos indica el aporte de la cantidad de agua en la altura de la planta

$z$ = cantidad de luz

$b_2$ = coeficiente que nos indica el aporte de la cantidad de luz en la altura de la planta

Ya que en nuestro universo nosotros conocemos la verdadera relación matemática que genera los datos que observamos, podemos calcular los valores "reales" que tendría la variable $y$ (la altura de la planta) en cada una de nuestras combinaciones de valores de agua y luz. Para esto aprovechamos las facilidades que nos brindan las operaciones vectoriales de R.

```{r}
#vector de tamaño n de una distribución uniforme con valores entre 5 y 9
z <- runif(n, min = 5, max = 9)

#vectores de tamaño n generados por una distribución normal con los siguientes parámetros:
b1 <- rnorm(n, mean = 5, sd = 0.7)
b2 <- rnorm(n, mean = 0.3, sd = 0.1)

y <- b1*w + b2*z

df <- data.frame(
  y = y,
  w = w,
  z = z
)

head(df)
```

Con histogramas podemos ver sus distribuciones:

```{r message=FALSE, warning=FALSE}
hist(df$y, breaks = 20)
hist(df$w, breaks = 20)
hist(df$z, breaks = 20)
```

Ya que conocemos los valores reales de nuestros coeficientes ($a$ y $b$), podemos saber facilmente cuál valor tendría $y$ (la altura de la planta) con una combinación cualquiera de $w$ y $z$ (agua y luz).

```{r}
w1 = 3
z1 = 9


mean(b1*w1 + b2*z1)
```

Este sería el valor real de la altura de la planta con 3 unidades de agua y 9 de luz en nuestro universo de datos generados.

## Inferencia estadística tradicional:

En estadística inferencial, normalmente estimamos los valores de los coeficientes de las variables ($b_1$ y $b_2$) y evaluamos qué tan probable es que nuestras estimaciones hayan sido generadas por la variación de las observaciones. Ya que nosotros mismos generamos los datos y conocemos los valores reales de estos coeficientes, podremos comparar qué tanto se acercan los valores de las estimaciones a los valores reales.

Usando una regresión lineal:

```{r}
mod1 <- lm(y ~ w + z, df)
mod1 |> summary()
```

Viendo el resumen de la regresión podemos notar que los coeficientes (\*Estimate\*) se acercan a nuestros valores reales.

Luego de estimarse los coeficientes, podemos hacer una predicción:

```{r}
predict(mod1, 
        data.frame(w = w1, z = z1))

```

Vease la cercanía con nuestro valor verdadero.

## Inferencia bayesiana:

En inferencia bayesiana es necesario plantear un modelo generador de datos y este debe ajustarse al conocimiento que tengamos del proceso que pensemos ha generado los datos observados. En nuestro caso utilizamos el siguiente:

$$
y \sim Normal(\mu, \sigma) \\
\mu = b_1w + b_2z \\
b_1 \sim Uniform(0,15) \\
b_2 \sim Uniform(0,15) \\
\sigma \sim Exp(1)
$$

El modelo se lee: $y$ proviene de una distribución normal donde su promedio está en función de una regresión lineal de $w$ y $z$.

Notese que a nuestras incógnitas ($\sigma$, $b_1$ y $b_2$ ) les asignamos distribuciones de "posibles valores" (a estas distribuciones también se le llaman "previas"). Durante el ajuste del modelo, él se encargará de encontrar las distribuciones más adecuadas según nuestros datos.

El modelo ahora debe ser codificado y en este caso usamos la sintaxis de la librería Rethinking. Esta librería "traduce" el código desde R al lenguaje del programa Stan y lo ejecuta.

```{r message=FALSE, warning=FALSE}
mod2 <- ulam(
  alist(
    y ~ dnorm(mu, sig),
    mu <- b1*w + b2*z,
    b1 ~ dunif(0, 15),
    b2 ~ dunif(0, 15),
    sig ~ dexp(1)
  ), data = df, chains = 4, cores = 4, cmdstan = TRUE
)
```

Luego de que el modelo ha terminado de ajustarse, podemos ver los valores de nuestras incógnitas (distribución posterior) tanto puntualmente como a manera de distribuciones:

```{r}
precis(mod2)
plot(precis(mod2))

smpls <- extract.samples(mod2)
hist(smpls$b1)

```

Algo característico de la inferencia bayesiana es que nuestras estimaciones no son puntuales, si no distribuciones de posibles valores según lo establecido en el modelo generador de datos. También es importante notar cómo a pesar de haber usado previas no informativas (cualquier valor posible entre dos límites), al calcular la posterior el modelo logró muy bien actualizarlas a valores posibles muy cercanos a la realidad. Esta es la "actualización bayesiana", de cómo nuestro conocimiento previo (distribuciones previas) se actualiza según la evidencia presentada (datos).

### Ahora la predicción:

Ya que en el modelo bayesiano todas nuestras incógnitas son distribuciones, al calcular una predicción el modelo usa muestras de los posibles valores de las incógnitas ($b_1$ y $b_2$) actualizadas según nuestros datos. Calcular la predicción utilizando los posibles valores de nuestras incógnitas facilita mostrar la incertidumbre de las predicciones.

```{r}
y_hat_sim <- sim(mod2, 
             data.frame(w = 3, z = 9))
head(y_hat_sim)
hist(y_hat_sim, breaks = 20)
mean(y_hat_sim)
quantile(y_hat_sim, probs = c(0.05, 0.5, 0.95))
```

Ahora no solo tenemos una estimación puntual del posible valor de nuestra predicción, sino toda una distribución de posibles valores. Esto da la posibilidad de mostrar con certeza rangos de valores dentro de los cuales estarían los valores más probables de nuestra predicción.

Utilizar valores simulados de nuestra distribución posterior también nos permite evaluar la probabilidad de tener valores mayores o menores de cierto umbral.

Por ejemplo, en este caso es muy sencillo calcular la probabilidad de que el valor $y$ para $w$ y $z$ iguales a 3 y 9 sea mayor a 10:

```{r}
sum(y_hat_sim > 10) / length(y_hat_sim)
```

O que esté entre 10 y 20:

```{r}
sum(y_hat_sim > 10 & y_hat_sim < 20) / length(y_hat_sim)
```

El código de Stan que genera la librería Rethinking se puede ver con la función *stancode*:

```{r}
stancode(mod2)
```

------------------------------------------------------------------------

Una pequeña explicación práctica de qué es la máxima verosimilitud (**maximum likelihood**):

Para esto nos planteamos las preguntas: ¿Cuanto varía la verosimilitud para un valor $t$ si cambiamos los valores de los parámetros de nuestro modelo generador de datos? y ¿cuál de estos nuevos valores de los parámetros me da la verosimilitud mayor para $t$?. En otras palabras, ¿cuál valor de los parámetros de nuestro modelo generador de datos sería el que con mayor certeza me generó este dato específico?

En el ejemplo lo hacemos cambiando el valor del promedio de una distribución normal. (Recordemos que $t$ es un valor generado de una distribución normal de promedio 7 y desviación estándard 1.5 y estamos tratando de *inferir* cuáles son los parámetros de esa distribución *desconocida*).

```{r}
t

#secuencia de valores enteros entre 3 y 17 con los que calcularemos la verosimilitud para t
mean_eval <- seq(from = 3, to = 17)

likelihood <- dnorm(t, mean = mean_eval, sd = 1.5) |> 
  (\(x) round(x, 5))()

df <- data.frame(mean_eval = mean_eval, likelihood = likelihood)

barplot(height = df$likelihood, names.arg = df$mean_eval)

df[which.max(df$likelihood),]

```

Vemos que para un solo valor, los parámetros del modelo generador puede que no sean los más probables.

¿Y qué tal si en vez de evaluar un solo valor, calculamos la máxima verosimilitud para un vector de valores producto de nuestro modelo generador de datos?

```{r}

#creamos un vector de n valores (400) de una distribución normal con parametros 7 y 1.5
t_vec <- rnorm(n, mean = 7, sd = 1.5)

#creamos una función que evaluará los diferentes valores del promedio mean_eval para cada elemento del vector t_vec
likelihood_fun <- function(x) dnorm(t_vec, mean = x, sd = 1.5)

#aplicamos la función a cada valor mean_eval (secuencia de valores enteros entre 3 y 17)
#la lista resultante contiene 15 elementos (mean_eval) y cada uno de esos elementos contiene las 400 verosimilitudes (n) según el mean_eval específico
lklhd_l <- lapply(mean_eval, likelihood_fun)

#calculamos el promedio de la verosimilitud para cada uno de los 15 valores de mean_eval
lklhd_mean <- lapply(lklhd_l, mean) |> unlist()

#y la convertimos en un dataframe con su respectivo valor de mean_eval
df <- data.frame(mean_eval = mean_eval, 
                 mean_likelihood = lklhd_mean)

barplot(height = df$mean_likelihood, names.arg = df$mean_eval)

df[which.max(df$mean_likelihood),]

```

Vemos que cuando tomamos en cuenta un serie de valores producidos por nuestro modelo generador de datos, la máxima verosimilitud para el parámetro que evaluamos refleja el valor real con el que se generaron los datos. En este caso vemos que de los 15 valores con que evaluamos el parámetro del promedio de nuestro modelo generador de datos (usando el vector de 400 elementos), el 7 resultó ser el que tiene la mayor verosimilitud para el promedio de la distribución normal.

Con las funciones de densidad de las distribuciones también podemos graficar sus *pdf*s (*probability density functions*):

```{r}
x_norm <- seq(0, 15, by = .1)
y_norm <- dnorm(x_norm, mean = 7, sd = 1.5)
plot(x_norm, y_norm)

dnorm(9, mean = 7, sd = 1.5)
```

------------------------------------------------------------------------

Y finalmente, todo esto viene del teorema de Bayes:

$$
p(h|d) = \frac{p(d|h) p(h)}{p(d)}
$$
