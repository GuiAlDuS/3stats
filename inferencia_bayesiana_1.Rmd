---
title: "Introducción a la inferencia bayesiana"
author: "Guillermo A. Durán"
date: "28 de setiembre de 2021"
output:
  html_document:
    df_print: paged
---

```{r message=FALSE, warning=FALSE}
library(rethinking)
set.seed(1234)
```

Este notebook es una introducción práctica muy general a la inferencia bayesiana. Presenta la forma en que me habría gustado recibir una introducción a la inferencia bayesiana y hace eco al enfoque pedagógico del libro [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/). El notebook está escrito en R y usa la librería [Rethinking](https://github.com/rmcelreath/rethinking) para hacer inferencia bayesiana.

El notebook hace énfasis en simulaciones usando modelos generadores de datos. Estas simulaciones nos permiten conocer de antemano el proceso que genera nuestros datos/observaciones. Seguidamente se reproduce el procedimiento del analista, donde a través de inferencia estadística trata de reconstruir el proceso que generó las observaciones. Al haber creado nosotros mismos los datos, estos ejemplos dan la posibilidad de comparar la inferencia hecha vs los valores reales usados para generar los datos. Además, en el notebook también se comparan las predicciones hechas en cada uno de los enfoques y las ventajas que nos brinda el enfoque bayesiano cuando queremos tomar en cuenta la incertidumbre de las predicciones.

## Interpretaciones (muy generales) de probabilidad:

-   Según el enfoque frecuentista: probabilidad es la proporción de un tipo de evento específico en una serie larga de observaciones de un fenómeno. Esta es la probabilidad que tradicionalmente nos han enseñado en los cursos generales de estadística. Es importante considerar que la repetibilidad de observaciones que supone esta definición implica condiciones que en ciertas situaciones resultan imposibles.

-   Según el enfoque bayesiano: probabilidad es un valor entre 0 y 1 que cuantifica nuestra incertidumbre sobre un fenómeno. Vemos que esta es una definición más abierta a fenómenos donde no se pueden repetir observaciones o donde el tipo de evento a observar aún no ha ocurrido pero que podemos asumir que tiene una probabilidad distinta a 0 y queremos estimarla (probabilidad de la explosión de una bomba nuclear mientras es transportada, por ejemplo).

Esta diferencia de interpretaciones tiene consecuencias importantes, no solo a nivel práctico si no inclusive a nivel filosófico.

## El modelo generador de datos:

El modelo generador de datos es una función capaz de producir valores.

Usamos la distribución normal como ejemplo de cómo generar un valor desde un modelo generador de datos. En este caso la distribución normal es nuestro modelo generador de datos.

```{r}
t <- rnorm(1, mean = 7, sd = 1.5)
t
```

Nuestro valor *t* es generado por una función con parámetros conocidos (los parámetros en el caso de una distribución normal son el promedio y desviación estándar). Cada vez que creamos un valor desde un modelo generador de datos, este valor fluctuará según las características propias del modelo.

El modelo generador de datos también nos permite tomar el camino opuesto: Dado un modelo generador de datos con parámetros establecidos, ¿qué tan probable es que un valor específico sea generado por él?. A esta estimación se le llama *verosimilitud* y se hablará de ella más adelante.

```{r}
dnorm(t, mean = 7, sd = 1.5)
```

### Generando nuestros datos: siendo los dueños de nuestro universo

Ahora en vez de crear un único valor, vamos a crear un vector con 400 valores desde nuestro modelo generador de datos.

Si exploramos ese vector vemos cómo se reflejan las características (parámetros: promedio y desviación estándar) de nuestro modelo generador de datos. Esto lo podemos ver tanto con su histograma como con el valor de los estadísticos.

```{r}
n = 400

w <- rnorm(n, mean = 7, sd = 1.5)

hist(w, breaks = 20)

cat(
  paste0("Promedio: ", round(mean(w), 2), 
         "\nDesviación estándard: ", round(sd(w), 2)))

```

#### Un ejemplo usando nuestro modelo generador de datos

Ahora pensemos que en nuestro universo la altura de una planta está dada por dos factores (variables) en una relación según la siguiente ecuación:

$$
y = b_1w + b_2z
$$

donde:

$y$ = altura de la planta

$w$ = cantidad de agua

$b_1$ = coeficiente que nos indica el aporte de la cantidad de agua en la altura de la planta

$z$ = cantidad de luz

$b_2$ = coeficiente que nos indica el aporte de la cantidad de luz en la altura de la planta

Ya que en nuestro universo nosotros conocemos la verdadera relación matemática que genera los datos que observamos, podemos calcular los valores "reales" que tendría la variable $y$ (la altura de la planta) en cada una de nuestras combinaciones de valores de agua y luz. Para esto aprovechamos las facilidades que nos brindan las operaciones vectoriales de R.

```{r}
#vector de tamaño n de una distribución uniforme con valores entre 5 y 9
z <- runif(n, min = 5, max = 9)

#vectores de tamaño n generados por una distribución normal con los siguientes parámetros:
b1 <- rnorm(n, mean = 5, sd = 0.7)
b2 <- rnorm(n, mean = 0.3, sd = 0.1)

y <- b1*w + b2*z

df <- data.frame(
  y = y,
  w = w,
  z = z
)

head(df)
```

Notese que los valores de $b_1$ y $b_2$ (los coeficientes que nos indican el aporte de cada variable) se generaron utilizando un *modelo generador de datos*, en este caso una distribución normal con parámetros establecidos. Por lo anterior, cada valor de $b$ variará según los parámetros del modelo que los generó. **El objetivo de la inferencia estadística será, a través de los datos observados, estimar el valor de los parámetros detrás del modelo (proceso) que los generó**. En el caso de nuestro ejemplo será estimar el promedio (y si se quiere también la desviación estándar) de la distribución normal que me generó los vectores de $b_1$ y $b_2$, o sea, si la inferencia es correcta, estos valores estimados se aproximarán a 5 y 0.3 respectivamente.

También podemos pensar que al hacer inferencia estadística invertimos el proceso de generar los datos: en vez de generar valores usando un modelo generador de datos con parámetros establecidos (siendo los dioses creadores de este universo), lo que haremos es, utilizando los valores generados por algún proceso que queremos entender, estimaremos los parámetros de las distribuciones que pensamos los generaron (somos personas que únicamente observamos los valores); y si utilizamos inferencia bayesiana, no solo estimamos esos valores si no que calculamos su incertidumbre (según la información/evidencia/datos con que contamos).

En el caso de nuestro ejemplo, podemos ver las distribuciones de los valores generados utilizando histogramas:

```{r message=FALSE, warning=FALSE}
hist(df$y, breaks = 20)
hist(df$w, breaks = 20)
hist(df$z, breaks = 20)
```

Ya que conocemos los valores reales de nuestros coeficientes ($b_1$ y $b_2$), podemos saber fácilmente, en promedio, cuál valor tendría $y$ (la altura de la planta) con una combinación cualquiera de $w$ y $z$ (agua y luz).

```{r}
w1 = 3
z1 = 9


mean(b1*w1 + b2*z1)
```

Este sería la altura promedio de las plantas cuando les aplicamos 3 unidades de agua y 9 de luz en nuestro universo de datos generados.

## Inferencia estadística tradicional:

Como se mencionó anteriormente, en estadística inferencial normalmente estimamos los valores de los coeficientes de las variables ($b_1$ y $b_2$) y evaluamos qué tan probable es que nuestras estimaciones hayan sido generadas por azar (significancia estadística). Ya que en este ejemplo nosotros mismos generamos los datos y conocemos los valores reales de estos coeficientes, podremos comparar qué tanto se acercan los valores de las estimaciones a los valores reales.

Usando una regresión lineal:

```{r}
mod1 <- lm(y ~ w + z, df)
mod1 |> summary()
```

Viendo el resumen de la regresión podemos notar que los coeficientes (*Estimate* para $w$ y $z$ ) se acercan en cierto grado a nuestros valores reales y son "estadísticamente significativos" (*Pr* menores a 0.05). Existe una discusión muy larga sobre significancia estadística en métodos frecuentistas que evitaré.

Luego de estimarse los coeficientes, también podemos hacer una predicción:

```{r}
predict(mod1, 
        data.frame(w = w1, z = z1))

```

Véase la cercanía con nuestro valor promedio real (17.64).

## Inferencia bayesiana:

En inferencia bayesiana es necesario plantear un modelo generador de datos y este debe ajustarse al conocimiento que tengamos del proceso que pensemos ha generado los datos observados. En nuestro caso utilizamos el siguiente:

$$
y \sim Normal(\mu, \sigma) \\
\mu = b_1w + b_2z \\
b_1 \sim Uniform(0,15) \\
b_2 \sim Uniform(0,15) \\
\sigma \sim Exp(1)
$$

El modelo se lee: $y$ proviene de una distribución normal donde su promedio está en función de una regresión lineal de las variables $w$ y $z$. Los coeficientes $b_1$ y $b_2$ provienen de distribuciones uniformes con valores entre 0 y 15, mientras que $\sigma$ de una distribución exponencial con parámetro igual a 1 (solo genera valores positivos).

A la primera línea del modelo también se le llama *verosimilitud* (*likelihood*) y crearla plantea la dificultad de escoger la distribución que pensamos genera nuestros datos. Para esto varios autores aconsejan guiarnos por las distribuciones que nos reduzcan la cantidad de supuestos según el conocimiento que tenemos del proceso que estamos estudiando ([principio de máxima entropía](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy)). En el caso de fenómenos donde lo único que conocemos es que los valores que se producen tienen una varianza finita, la distribución normal es la que nos plantea menos supuestos.

Notese que a nuestras incógnitas ($\sigma$, $b_1$ y $b_2$ ) les asignamos distribuciones de "posibles valores" (a estas distribuciones también se le llaman "previas"). Durante el ajuste del modelo, él se encargará de encontrar las distribuciones más adecuadas según nuestros datos.

El modelo ahora debe ser codificado y en este caso usamos la sintaxis de la librería [Rethinking](https://github.com/rmcelreath/rethinking). Esta librería "traduce" el código desde R al lenguaje del programa Stan y lo ejecuta.

```{r message=FALSE, warning=FALSE}
mod2 <- ulam(
  alist(
    y ~ dnorm(mu, sig),
    mu <- b1*w + b2*z,
    b1 ~ dunif(0, 15),
    b2 ~ dunif(0, 15),
    sig ~ dexp(1)
  ), data = df, chains = 4, cores = 4, cmdstan = TRUE
)
```

Luego de que el modelo ha terminado de ajustarse, podemos ver los valores de nuestras incógnitas (distribución posterior) tanto como estadísticos como también distribuciones:

```{r}
precis(mod2)
plot(precis(mod2))

smpls <- extract.samples(mod2)
hist(smpls$b1)

```

Algo característico de la inferencia bayesiana es que nuestras estimaciones no son puntuales, si no distribuciones de posibles valores según lo establecido en nuestro modelo generador de datos. También es importante notar cómo a pesar de haber usado previas no informativas (distribuciones uniformes donde cualquier valor entre dos límites tiene la misma probabilidad), al calcular la posterior el modelo logró muy bien aproximar las estimaciones a valores posibles muy cercanos a la realidad. **Esta es la "actualización bayesiana", de cómo nuestro conocimiento previo (distribuciones previas) se actualiza según la evidencia observada (datos).**

Además, es muy importante recalcar que con estas distribuciones podemos calcular rangos de certeza (intervalos de credibilidad). Entre estos valores la evidencia nos indica con cierta probabilidad dónde puede estar el valor del parámetro (salidas de la función *precis()*). En nuestro ejemplo, vemos cómo los rangos con un 89% de credibilidad de los coeficientes estimados (el rango que está entre los percentiles 4.5% y 94.5% de la distribución posterior) cubren los valores reales (5 para $b_1$ y 0.3 para $b_2$), inclusive están muy cerca de los promedios de las distribuciones posteriores.

### Ahora la predicción:

Ya que en el modelo bayesiano todas nuestras incógnitas y estimaciones son distribuciones, al calcular una predicción el modelo usa muestras de las distribuciones estimadas ($b_1$ y $b_2$) . Calcular la predicción utilizando los posibles valores de nuestras incógnitas facilita mostrar la incertidumbre de las predicciones.

```{r}
y_hat_sim <- sim(mod2, 
             data.frame(w = 3, z = 9))
head(y_hat_sim)
hist(y_hat_sim, breaks = 20)
mean(y_hat_sim)
quantile(y_hat_sim, probs = c(0.05, 0.5, 0.95))
```

Ahora no solo tenemos una estimación puntual del posible valor de nuestra predicción, sino toda una distribución de posibles valores. Esto da la posibilidad de mostrar con certeza rangos de valores dentro de los cuales estarían los valores más probables de nuestra predicción.

Utilizar valores simulados de nuestra distribución posterior también nos permite evaluar la probabilidad de tener valores mayores o menores de cierto umbral.

Por ejemplo, en este caso es muy sencillo calcular la probabilidad de que el valor $y$ para $w$ y $z$ iguales a 3 y 9 sea mayor a 10:

```{r}
sum(y_hat_sim > 10) / length(y_hat_sim)
```

O que esté entre 10 y 20:

```{r}
sum(y_hat_sim > 10 & y_hat_sim < 20) / length(y_hat_sim)
```

El código de Stan que genera la librería Rethinking se puede ver con la función *stancode*:

```{r}
stancode(mod2)
```

------------------------------------------------------------------------

Ahora una pequeña explicación práctica de qué es la máxima verosimilitud (**maximum likelihood**):

Para esto nos planteamos las preguntas: ¿Cuánto varía la verosimilitud para un valor $t$ si cambiamos los valores de los parámetros de nuestro modelo generador de datos? y ¿cuál de estos nuevos valores de los parámetros me da la verosimilitud mayor para $t$?. En otras palabras, ¿cuál valor de los parámetros de nuestro modelo generador de datos sería el que con mayor certeza me generó este dato específico?

En el ejemplo lo hacemos cambiando el valor del promedio de una distribución normal. (Recordemos que $t$ es un valor generado de una distribución normal de promedio 7 y desviación estándar 1.5, y estamos tratando de *inferir* cuáles son los parámetros de esa distribución *desconocida*).

```{r}
t

#secuencia de valores enteros entre 3 y 17 con los que calcularemos la verosimilitud para t
mean_eval <- seq(from = 3, to = 17)

likelihood <- dnorm(t, mean = mean_eval, sd = 1.5) |> 
  (\(x) round(x, 5))()

df <- data.frame(mean_eval = mean_eval, likelihood = likelihood)

barplot(height = df$likelihood, names.arg = df$mean_eval)

df[which.max(df$likelihood),]

```

Vemos que para un solo valor, los parámetros del modelo generador puede que no sean los más probables.

¿Y qué tal si en vez de evaluar un solo valor, calculamos la máxima verosimilitud para un vector de valores producto de nuestro modelo generador de datos?

```{r}

#creamos un vector de n valores (400) de una distribución normal con parametros 7 y 1.5
t_vec <- rnorm(n, mean = 7, sd = 1.5)

#creamos una función que evaluará los diferentes valores del promedio mean_eval para cada elemento del vector t_vec
likelihood_fun <- function(x) dnorm(t_vec, mean = x, sd = 1.5)

#aplicamos la función a cada valor mean_eval (secuencia de valores enteros entre 3 y 17)
#la lista resultante contiene 15 elementos (mean_eval) y cada uno de esos elementos contiene las 400 verosimilitudes (n) según el mean_eval específico
lklhd_l <- lapply(mean_eval, likelihood_fun)

#calculamos el promedio de la verosimilitud para cada uno de los 15 valores de mean_eval
lklhd_mean <- lapply(lklhd_l, mean) |> unlist()

#y la convertimos en un dataframe con su respectivo valor de mean_eval
df <- data.frame(mean_eval = mean_eval, 
                 mean_likelihood = lklhd_mean)

barplot(height = df$mean_likelihood, names.arg = df$mean_eval)

df[which.max(df$mean_likelihood),]

```

Vemos que cuando tomamos en cuenta un serie de valores producidos por nuestro modelo generador de datos, la máxima verosimilitud para el parámetro que evaluamos refleja el valor real con el que se generaron los datos. En este caso vemos que de los 15 valores con que evaluamos el parámetro del promedio de nuestro modelo generador de datos (usando el vector de 400 elementos), el 7 resultó ser el que tiene la mayor verosimilitud para el promedio de la distribución normal.

```{=html}
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Licencia de Creative Commons" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />Este obra está bajo una <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">licencia de Creative Commons Reconocimiento-NoComercial-CompartirIgual 4.0 Internacional</a>.
```
